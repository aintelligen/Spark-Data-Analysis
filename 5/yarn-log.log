spark.SparkContext: Running Spark version 2.3.0
spark.SparkContext: Submitted application: sparl041
spark.SecurityManager: Changing view acls to: hadoop
spark.SecurityManager: Changing modify acls to: hadoop
spark.SecurityManager: Changing view acls groups to: 
spark.SecurityManager: Changing modify acls groups to: 
spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()
util.Utils: Successfully started service 'sparkDriver' on port 58738.
spark.SparkEnv: Registering MapOutputTracker
spark.SparkEnv: Registering BlockManagerMaster
storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
storage.DiskBlockManager: Created local directory at /tmp/blockmgr-53e1ff5b-3774-4ea6-8766-8a9a49129fb3
memory.MemoryStore: MemoryStore started with capacity 366.3 MB
spark.SparkEnv: Registering OutputCommitCoordinator
util.log: Logging initialized @2042ms
server.Server: jetty-9.3.z-SNAPSHOT
server.Server: Started @2110ms
server.AbstractConnector: Started ServerConnector@1f277f8{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
util.Utils: Successfully started service 'SparkUI' on port 4040.
handler.ContextHandler: Started o.s.j.s.ServletContextHandler@498363d0{/jobs,null,AVAILABLE,@Spark}
handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41ccd43c{/jobs/json,null,AVAILABLE,@Spark}
handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b76f19{/jobs/job,null,AVAILABLE,@Spark}
handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5bc1ae85{/jobs/job/json,null,AVAILABLE,@Spark}
handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6431dd43{/stages,null,AVAILABLE,@Spark}
handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5dc91b37{/stages/json,null,AVAILABLE,@Spark}
handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7e21431a{/stages/stage,null,AVAILABLE,@Spark}
handler.ContextHandler: Started o.s.j.s.ServletContextHandler@57f6df99{/stages/stage/json,null,AVAILABLE,@Spark}
handler.ContextHandler: Started o.s.j.s.ServletContextHandler@23bb6fee{/stages/pool,null,AVAILABLE,@Spark}
handler.ContextHandler: Started o.s.j.s.ServletContextHandler@18bc50d3{/stages/pool/json,null,AVAILABLE,@Spark}
handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41b4ee7b{/storage,null,AVAILABLE,@Spark}
handler.ContextHandler: Started o.s.j.s.ServletContextHandler@20dffbc3{/storage/json,null,AVAILABLE,@Spark}
handler.ContextHandler: Started o.s.j.s.ServletContextHandler@58632429{/storage/rdd,null,AVAILABLE,@Spark}
handler.ContextHandler: Started o.s.j.s.ServletContextHandler@22a3dd3e{/storage/rdd/json,null,AVAILABLE,@Spark}
handler.ContextHandler: Started o.s.j.s.ServletContextHandler@44823827{/environment,null,AVAILABLE,@Spark}
handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b5cd297{/environment/json,null,AVAILABLE,@Spark}
handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5a5fefb9{/executors,null,AVAILABLE,@Spark}
handler.ContextHandler: Started o.s.j.s.ServletContextHandler@66a993e1{/executors/json,null,AVAILABLE,@Spark}
handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33bfdc13{/executors/threadDump,null,AVAILABLE,@Spark}
handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4e0a7899{/executors/threadDump/json,null,AVAILABLE,@Spark}
handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6b0d74c6{/static,null,AVAILABLE,@Spark}
handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e746ae1{/,null,AVAILABLE,@Spark}
handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2eafa2fe{/api,null,AVAILABLE,@Spark}
handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ff9be6a{/jobs/job/kill,null,AVAILABLE,@Spark}
handler.ContextHandler: Started o.s.j.s.ServletContextHandler@57755685{/stages/stage/kill,null,AVAILABLE,@Spark}
ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://hadoop000:4040
executor.Executor: Starting executor ID driver on host localhost
util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51302.
netty.NettyBlockTransferService: Server created on hadoop000:51302
storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, hadoop000, 51302, None)
storage.BlockManagerMasterEndpoint: Registering block manager hadoop000:51302 with 366.3 MB RAM, BlockManagerId(driver, hadoop000, 51302, None)
storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, hadoop000, 51302, None)
storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, hadoop000, 51302, None)
handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1f685aef{/metrics/json,null,AVAILABLE,@Spark}
scheduler.EventLoggingListener: Logging events to hdfs://hadoop000:8020/directory/local-1551931008342
memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 226.4 KB, free 366.1 MB)
memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 21.3 KB, free 366.1 MB)
storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop000:51302 (size: 21.3 KB, free: 366.3 MB)
spark.SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0
mapred.FileInputFormat: Total input paths to process : 1
spark.SparkContext: Starting job: sortByKey at /home/hadoop/script/spark0501.py:20
scheduler.DAGScheduler: Registering RDD 3 (reduceByKey at /home/hadoop/script/spark0501.py:18)
scheduler.DAGScheduler: Got job 0 (sortByKey at /home/hadoop/script/spark0501.py:20) with 2 output partitions
scheduler.DAGScheduler: Final stage: ResultStage 1 (sortByKey at /home/hadoop/script/spark0501.py:20)
scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 0)
scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /home/hadoop/script/spark0501.py:18), which has no missing parents
memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.6 KB, free 366.0 MB)
memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.2 KB, free 366.0 MB)
storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hadoop000:51302 (size: 6.2 KB, free: 366.3 MB)
spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1039
scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /home/hadoop/script/spark0501.py:18) (first 15 tasks are for partitions Vector(0, 1))
scheduler.TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 7871 bytes)
scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 7871 bytes)
executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
rdd.HadoopRDD: Input split: hdfs://hadoop000:8020/page_view.txt:905+906
rdd.HadoopRDD: Input split: hdfs://hadoop000:8020/page_view.txt:0+905
python.PythonRunner: Times: total = 652, boot = 418, init = 153, finish = 81
python.PythonRunner: Times: total = 596, boot = 491, init = 28, finish = 77
executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 1565 bytes result sent to driver
executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 1522 bytes result sent to driver
scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1500 ms on localhost (executor driver) (1/2)
scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1498 ms on localhost (executor driver) (2/2)
scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
scheduler.DAGScheduler: ShuffleMapStage 0 (reduceByKey at /home/hadoop/script/spark0501.py:18) finished in 1.645 s
scheduler.DAGScheduler: looking for newly runnable stages
scheduler.DAGScheduler: running: Set()
scheduler.DAGScheduler: waiting: Set(ResultStage 1)
scheduler.DAGScheduler: failed: Set()
scheduler.DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at sortByKey at /home/hadoop/script/spark0501.py:20), which has no missing parents
memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 7.4 KB, free 366.0 MB)
memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.8 KB, free 366.0 MB)
storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on hadoop000:51302 (size: 4.8 KB, free: 366.3 MB)
spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1039
scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (PythonRDD[6] at sortByKey at /home/hadoop/script/spark0501.py:20) (first 15 tasks are for partitions Vector(0, 1))
scheduler.TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7649 bytes)
scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 7649 bytes)
executor.Executor: Running task 0.0 in stage 1.0 (TID 2)
executor.Executor: Running task 1.0 in stage 1.0 (TID 3)
storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
python.PythonRunner: Times: total = 33, boot = -422, init = 454, finish = 1
python.PythonRunner: Times: total = 32, boot = -450, init = 481, finish = 1
executor.Executor: Finished task 0.0 in stage 1.0 (TID 2). 1568 bytes result sent to driver
executor.Executor: Finished task 1.0 in stage 1.0 (TID 3). 1611 bytes result sent to driver
scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 121 ms on localhost (executor driver) (1/2)
scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 121 ms on localhost (executor driver) (2/2)
scheduler.DAGScheduler: ResultStage 1 (sortByKey at /home/hadoop/script/spark0501.py:20) finished in 0.145 s
scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
scheduler.DAGScheduler: Job 0 finished: sortByKey at /home/hadoop/script/spark0501.py:20, took 2.008161 s
spark.SparkContext: Starting job: sortByKey at /home/hadoop/script/spark0501.py:20
scheduler.DAGScheduler: Got job 1 (sortByKey at /home/hadoop/script/spark0501.py:20) with 2 output partitions
scheduler.DAGScheduler: Final stage: ResultStage 3 (sortByKey at /home/hadoop/script/spark0501.py:20)
scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
scheduler.DAGScheduler: Missing parents: List()
scheduler.DAGScheduler: Submitting ResultStage 3 (PythonRDD[7] at sortByKey at /home/hadoop/script/spark0501.py:20), which has no missing parents
memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 7.5 KB, free 366.0 MB)
memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.9 KB, free 366.0 MB)
storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hadoop000:51302 (size: 4.9 KB, free: 366.3 MB)
spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1039
scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 3 (PythonRDD[7] at sortByKey at /home/hadoop/script/spark0501.py:20) (first 15 tasks are for partitions Vector(0, 1))
scheduler.TaskSchedulerImpl: Adding task set 3.0 with 2 tasks
scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 4, localhost, executor driver, partition 0, ANY, 7649 bytes)
scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 5, localhost, executor driver, partition 1, ANY, 7649 bytes)
executor.Executor: Running task 0.0 in stage 3.0 (TID 4)
executor.Executor: Running task 1.0 in stage 3.0 (TID 5)
storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
python.PythonRunner: Times: total = 50, boot = -130, init = 180, finish = 0
executor.Executor: Finished task 0.0 in stage 3.0 (TID 4). 1590 bytes result sent to driver
scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 4) in 82 ms on localhost (executor driver) (1/2)
python.PythonRunner: Times: total = 85, boot = -130, init = 214, finish = 1
executor.Executor: Finished task 1.0 in stage 3.0 (TID 5). 1568 bytes result sent to driver
scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 5) in 116 ms on localhost (executor driver) (2/2)
scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
scheduler.DAGScheduler: ResultStage 3 (sortByKey at /home/hadoop/script/spark0501.py:20) finished in 0.139 s
scheduler.DAGScheduler: Job 1 finished: sortByKey at /home/hadoop/script/spark0501.py:20, took 0.142917 s
spark.SparkContext: Starting job: runJob at PythonRDD.scala:141
scheduler.DAGScheduler: Registering RDD 9 (sortByKey at /home/hadoop/script/spark0501.py:20)
scheduler.DAGScheduler: Got job 2 (runJob at PythonRDD.scala:141) with 1 output partitions
scheduler.DAGScheduler: Final stage: ResultStage 6 (runJob at PythonRDD.scala:141)
scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 5)
scheduler.DAGScheduler: Submitting ShuffleMapStage 5 (PairwiseRDD[9] at sortByKey at /home/hadoop/script/spark0501.py:20), which has no missing parents
memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 8.2 KB, free 366.0 MB)
memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.5 KB, free 366.0 MB)
storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop000:51302 (size: 5.5 KB, free: 366.3 MB)
spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1039
scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (PairwiseRDD[9] at sortByKey at /home/hadoop/script/spark0501.py:20) (first 15 tasks are for partitions Vector(0, 1))
scheduler.TaskSchedulerImpl: Adding task set 5.0 with 2 tasks
scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6, localhost, executor driver, partition 0, ANY, 7638 bytes)
scheduler.TaskSetManager: Starting task 1.0 in stage 5.0 (TID 7, localhost, executor driver, partition 1, ANY, 7638 bytes)
executor.Executor: Running task 1.0 in stage 5.0 (TID 7)
executor.Executor: Running task 0.0 in stage 5.0 (TID 6)
storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
python.PythonRunner: Times: total = 56, boot = -161, init = 217, finish = 0
executor.Executor: Finished task 1.0 in stage 5.0 (TID 7). 1737 bytes result sent to driver
python.PythonRunner: Times: total = 52, boot = -195, init = 247, finish = 0
scheduler.TaskSetManager: Finished task 1.0 in stage 5.0 (TID 7) in 80 ms on localhost (executor driver) (1/2)
executor.Executor: Finished task 0.0 in stage 5.0 (TID 6). 1737 bytes result sent to driver
scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 6) in 84 ms on localhost (executor driver) (2/2)
scheduler.TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
scheduler.DAGScheduler: ShuffleMapStage 5 (sortByKey at /home/hadoop/script/spark0501.py:20) finished in 0.118 s
scheduler.DAGScheduler: looking for newly runnable stages
scheduler.DAGScheduler: running: Set()
scheduler.DAGScheduler: waiting: Set(ResultStage 6)
scheduler.DAGScheduler: failed: Set()
scheduler.DAGScheduler: Submitting ResultStage 6 (PythonRDD[12] at RDD at PythonRDD.scala:48), which has no missing parents
memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 6.8 KB, free 366.0 MB)
memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.5 KB, free 366.0 MB)
storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop000:51302 (size: 4.5 KB, free: 366.3 MB)
spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1039
scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (PythonRDD[12] at RDD at PythonRDD.scala:48) (first 15 tasks are for partitions Vector(0))
scheduler.TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8, localhost, executor driver, partition 0, ANY, 7649 bytes)
executor.Executor: Running task 0.0 in stage 6.0 (TID 8)
storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 2 blocks
storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
python.PythonRunner: Times: total = 55, boot = -45, init = 100, finish = 0
executor.Executor: Finished task 0.0 in stage 6.0 (TID 8). 1590 bytes result sent to driver
scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 85 ms on localhost (executor driver) (1/1)
scheduler.TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
scheduler.DAGScheduler: ResultStage 6 (runJob at PythonRDD.scala:141) finished in 0.100 s
scheduler.DAGScheduler: Job 2 finished: runJob at PythonRDD.scala:141, took 0.232155 s
spark.SparkContext: Starting job: runJob at PythonRDD.scala:141
scheduler.DAGScheduler: Got job 3 (runJob at PythonRDD.scala:141) with 1 output partitions
scheduler.DAGScheduler: Final stage: ResultStage 9 (runJob at PythonRDD.scala:141)
scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)
scheduler.DAGScheduler: Missing parents: List()
scheduler.DAGScheduler: Submitting ResultStage 9 (PythonRDD[13] at RDD at PythonRDD.scala:48), which has no missing parents
memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 6.8 KB, free 366.0 MB)
memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 4.5 KB, free 366.0 MB)
storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop000:51302 (size: 4.5 KB, free: 366.2 MB)
spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1039
scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (PythonRDD[13] at RDD at PythonRDD.scala:48) (first 15 tasks are for partitions Vector(1))
scheduler.TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 1, ANY, 7649 bytes)
executor.Executor: Running task 0.0 in stage 9.0 (TID 9)
storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
python.PythonRunner: Times: total = 44, boot = -181, init = 225, finish = 0
executor.Executor: Finished task 0.0 in stage 9.0 (TID 9). 1656 bytes result sent to driver
scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 54 ms on localhost (executor driver) (1/1)
scheduler.TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
scheduler.DAGScheduler: ResultStage 9 (runJob at PythonRDD.scala:141) finished in 0.078 s
scheduler.DAGScheduler: Job 3 finished: runJob at PythonRDD.scala:141, took 0.081717 s
1.963.34.243: 5
1.963.34.203: 3
1.963.34.223: 2
1.963.34.213: 1
hdfs://hadoop000:8020/page_view.txt hdfs://hadoop000:8020/wc/output/2019-03-07_11-56-53
memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 226.4 KB, free 365.8 MB)
memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 21.3 KB, free 365.7 MB)
storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop000:51302 (size: 21.3 KB, free: 366.2 MB)
spark.SparkContext: Created broadcast 7 from textFile at NativeMethodAccessorImpl.java:0
Configuration.deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
output.FileOutputCommitter: File Output Committer Algorithm version is 1
mapred.FileInputFormat: Total input paths to process : 1
spark.SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78
scheduler.DAGScheduler: Got job 4 (runJob at SparkHadoopWriter.scala:78) with 2 output partitions
scheduler.DAGScheduler: Final stage: ResultStage 10 (runJob at SparkHadoopWriter.scala:78)
scheduler.DAGScheduler: Parents of final stage: List()
scheduler.DAGScheduler: Missing parents: List()
scheduler.DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[18] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents
memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 71.6 KB, free 365.7 MB)
memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 27.0 KB, free 365.6 MB)
storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on hadoop000:51302 (size: 27.0 KB, free: 366.2 MB)
spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1039
scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 10 (MapPartitionsRDD[18] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
scheduler.TaskSchedulerImpl: Adding task set 10.0 with 2 tasks
scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, ANY, 7882 bytes)
scheduler.TaskSetManager: Starting task 1.0 in stage 10.0 (TID 11, localhost, executor driver, partition 1, ANY, 7882 bytes)
executor.Executor: Running task 0.0 in stage 10.0 (TID 10)
executor.Executor: Running task 1.0 in stage 10.0 (TID 11)
rdd.HadoopRDD: Input split: hdfs://hadoop000:8020/page_view.txt:905+906
rdd.HadoopRDD: Input split: hdfs://hadoop000:8020/page_view.txt:0+905
output.FileOutputCommitter: File Output Committer Algorithm version is 1
output.FileOutputCommitter: File Output Committer Algorithm version is 1
python.PythonRunner: Times: total = 41, boot = -463, init = 504, finish = 0
python.PythonRunner: Times: total = 95, boot = -361, init = 456, finish = 0
output.FileOutputCommitter: Saved output of task 'attempt_20190307115654_0018_m_000000_0' to hdfs://hadoop000:8020/wc/output/_temporary/0/task_20190307115654_0018_m_000000
output.FileOutputCommitter: Saved output of task 'attempt_20190307115654_0018_m_000001_0' to hdfs://hadoop000:8020/wc/output/_temporary/0/task_20190307115654_0018_m_000001
mapred.SparkHadoopMapRedUtil: attempt_20190307115654_0018_m_000001_0: Committed
mapred.SparkHadoopMapRedUtil: attempt_20190307115654_0018_m_000000_0: Committed
executor.Executor: Finished task 0.0 in stage 10.0 (TID 10). 1701 bytes result sent to driver
scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 368 ms on localhost (executor driver) (1/2)
executor.Executor: Finished task 1.0 in stage 10.0 (TID 11). 1701 bytes result sent to driver
scheduler.TaskSetManager: Finished task 1.0 in stage 10.0 (TID 11) in 375 ms on localhost (executor driver) (2/2)
scheduler.TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
scheduler.DAGScheduler: ResultStage 10 (runJob at SparkHadoopWriter.scala:78) finished in 0.492 s
scheduler.DAGScheduler: Job 4 finished: runJob at SparkHadoopWriter.scala:78, took 0.494757 s
io.SparkHadoopWriter: Job job_20190307115654_0018 committed.
server.AbstractConnector: Stopped Spark@1f277f8{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
ui.SparkUI: Stopped Spark web UI at http://hadoop000:4040
spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
memory.MemoryStore: MemoryStore cleared
storage.BlockManager: BlockManager stopped
storage.BlockManagerMaster: BlockManagerMaster stopped
scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
spark.SparkContext: Successfully stopped SparkContext
util.ShutdownHookManager: Shutdown hook called
util.ShutdownHookManager: Deleting directory /tmp/spark-afc2bea0-e8b5-42aa-8e72-48343eb0e4d7
util.ShutdownHookManager: Deleting directory /tmp/spark-ca519a82-6f88-4229-9b6b-04ae4fca95ed/pyspark-96bdfcbf-1834-4033-8c79-239b3f785a06
util.ShutdownHookManager: Deleting directory /tmp/spark-ca519a82-6f88-4229-9b6b-04ae4fca95ed
