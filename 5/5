Local模式：
	开发

	--master
	--name
	--py-files


./spark-submit --master local[2] --name spark-local /home/hadoop/script/spark0402.py file:///home/hadoop/data/hello.txt file:///home/hadoop/wc/output


standalone 
启动 hdfs  和 spark
```shell
cd /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/sbin
./start-dfs.sh
./start-yarn.sh


cd /home/hadoop/app/spark-2.3.0-bin-2.6.0-cdh5.7.0/bin
./spark-shell
```
查看
http://hadoop000:50070/dfshealth.html#tab-overview
http://hadoop000:8088/cluster

http://hadoop000:4040/executors/



	hdfs: NameNode  DataNode SecondaryNameNode
	yarn: ResourceManager NodeManager

	master:
	worker:

	$SPARK_HOME/conf/slaves
		hadoop000

		假设你有5台机器，就应该进行如下slaves的配置
		hadoop000
		hadoop001
		hadoop002
		hadoop003
		hadoop005
		如果是多台机器，那么每台机器都在相同的路径下部署spark


	启动spark集群
		$SPARK_HOME/sbin/start-all.sh
		ps: 要在spark-env.sh中添加JAVA_HOME，否则会报错
		检测： 
			jps： Master和Worker进程，就说明我们的standalone模式安装成功
			webui：

		查看日志：
		ui.MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://hadoop000:8080 （Master）
		ui.WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://hadoop000:8081 （Worker）

		$SPARK_HOME/bin
		cd ~/app/spark-2.3.0-bin-2.6.0-cdh5.7.0/bin
		
		查看 http://hadoop000:8080  Running Applications
		点击进到作业中，提交作业，刷新UI（http://hadoop000:4041/jobs/   http://hadoop000:4040/jobs/）
		
		hadoop fs -ls /
		hadoop fs -put /home/hadoop/script/wc/page_view.txt /page_view.txt

		cd ~/app/spark-2.3.0-bin-2.6.0-cdh5.7.0/bin
		./pyspark --master spark://hadoop000:7077
		data = [1,2,3,4,5]
		sc.parallelize(data).collect()
		quit()

		cd ~/app/spark-2.3.0-bin-2.6.0-cdh5.7.0/sbin
    ./spark-submit --master spark://hadoop000:7077 --name spark-standalone /home/hadoop/script/spark0501.py hdfs://hadoop000:8020/page_view.txt hdfs://hadoop000:8020/wc/output

	如果使用standalone模式，而且你的节点个数大于1的时候，如果你使用本地文件测试，必须要保证每个节点上都有本地测试文件
	
	
yarn
	mapreduce yarn
	spark on yarn 70%
	spark作业客户端而已，他需要做的事情就是提交作业到yarn上去执行
	yarn vs standalone
		yarn： 你只需要一个节点，然后提交作业即可   这个是不需要spark集群的（不需要启动master和worker的）
		standalone：你的spark集群上每个节点都需要部署spark，然后需要启动spark集群（需要master和worker）




When running with master 'yarn' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment

vi /home/hadoop/app/spark-2.3.0-bin-2.6.0-cdh5.7.0/conf/spark-env.sh
HADOOP_CONF_DIR=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop

hadoop fs -rm -r -skipTrash /wc/output
./spark-submit --master yarn --name spark-yarn /home/hadoop/script/spark0501.py hdfs://hadoop000:8020/page_view.txt hdfs://hadoop000:8020/wc/output


作业：试想：为什么需要指定HADOOP_CONF_DIR或者YARN_CONF_DIR

如何使得这个信息规避掉
Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME

yarn支持client和cluster模式：driver运行在哪里
	client：提交作业的进程是不能停止的，否则作业就挂了
	cluster：提交完作业，那么提交作业端就可以断开了，因为driver是运行在am里面的


Error: Cluster deploy mode is not applicable to Spark shells

	pyspark/spark-shell : 交互式运行程序  client
	spark-sql

如何查看已经运行完的yarn的日志信息： yarn logs -applicationId <applicationId>
Log aggregation has not completed or is not enabled.
参见：https://coding.imooc.com/class/chapter/128.html#Anchor  JobHistory使用


不管你的spark应用程序运行在哪里，你的spark代码都是一样的，不需要做任何的修改和调整，所以spark使用起来是非常方便的！！！！！！



